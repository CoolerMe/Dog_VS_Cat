{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 这个文档是训练模型专用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "#加载数据的方法\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    print(type(data))\n",
    "    data_files = np.array(data['filenames'])\n",
    "    print(data_files[0])\n",
    "#     data_targets = np_utils.to_categorical(np.array(data['target']), 2)\n",
    "    data_targets = np.array(data['target'])\n",
    "    return data_files,data_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "train_set/cat/cat.3557.jpg\n"
     ]
    }
   ],
   "source": [
    "#加载所有的处理过的猫狗数据\n",
    "train_files,train_targets=load_dataset('train_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印下看看有没有问题\n",
    "print(train_files[0],train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意,不能直接加载数据集然后再分割,因为这样很占用内存\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=train_files\n",
    "Y=train_targets\n",
    "x_train,x_valid,y_train,y_valid=train_test_split(X,Y,test_size=0.2,random_state=8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import Model\n",
    "\n",
    "\n",
    "base_model=InceptionResNetV2(weights='imagenet',include_top=False)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "x=GlobalAveragePooling2D()(base_model.output)\n",
    "x=Dropout(0.2)(x)\n",
    "# 添加一个分类器，我们有2个类\n",
    "predictions=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# 构建我们需要训练的完整模型\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)\n",
    "    \n",
    "#输出模型的样式\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#展示模型,因为要安装额外软件,所以linux就注释掉了\n",
    "#from keras.utils import plot_model\n",
    "#pip install pydot\n",
    "#pip install pydot-ng\n",
    "#pip install graphviz \n",
    "#参考https://blog.csdn.net/u011311291/article/details/80298563\n",
    "#https://packages.ubuntu.com/search?keywords=graphviz&searchon=names\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一种可视化模型的方法\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来的处理方式是将所有的图片都处理成向量\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input as ir_preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "##pip install  tqdm\n",
    "#注意要安装tqdm\n",
    "def path_to_tensor(image_path,target_size=(299,299)):\n",
    "    '''将图片转换为tensor向量'''\n",
    "    img=image.load_img(image_path,target_size=target_size)\n",
    "    x=image.img_to_array(img)\n",
    "    x=ir_preprocess_input(x)\n",
    "    return np.expand_dims(x,axis=0)\n",
    "\n",
    "def paths_to_tensor(image_paths):\n",
    "    '''批量将图片转换为tensor'''\n",
    "    list_of_tensor=[path_to_tensor(image_path) for image_path in tqdm(image_paths)]\n",
    "    return np.vstack(list_of_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型看看\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "###设置训练模型的epochs的数量\n",
    "epochs = 30\n",
    "batch_size=50\n",
    "check_point_filepath='./saved_models/weights.best.from_scratch_adam.hdf5'\n",
    "saved_model_path='./saved_models'\n",
    "\n",
    "#文件夹不存在,就创建文件夹\n",
    "if not os.path.exists(saved_model_path):\n",
    "    os.mkdir()\n",
    "    \n",
    "####设置检查点\n",
    "checkpointer = ModelCheckpoint(filepath=check_point_filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "#设置回调\n",
    "call_back=[checkpointer]\n",
    "\n",
    "#加载数据\n",
    "x_train_data=paths_to_tensor(x_train)\n",
    "x_valid_data=paths_to_tensor(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练\n",
    "epochs=10\n",
    "history= model.fit(x_train_data,y_train,validation_data=(x_valid_data,y_valid),epochs=epochs,batch_size=batch_size,callbacks=call_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#展示Accuracy数据\n",
    "def show_acc(history):\n",
    "    print(history.history.keys())\n",
    "    figure=plt.figure()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accucary')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.ylabel('epoch')\n",
    "    #注意!\n",
    "    plt.legend(['train','valid'],loc='upper left')\n",
    "    figure.savefig('performance_acc.png')\n",
    "    \n",
    "\n",
    "#展示Loss数据  \n",
    "def show_loss(history):\n",
    "    print(history.history.keys())\n",
    "    figure=plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('epoch')\n",
    "    #注意!\n",
    "    plt.legend(['train','valid'],loc='upper left')\n",
    "    figure.savefig('performance_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show_history(history)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='lower left')\n",
    "\n",
    "# fig.savefig('performance.png')\n",
    "#展示训练后的数据\n",
    "show_acc(history)\n",
    "show_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目前遭遇了一个严重问题,在使用数据集5000,epoch=10,batch_size=50的时候,验证集的val_loss:保持在8.2847 ,val_acc:固定在0.4860,令人费解.\n",
    "#### 我做了如下尝试:\n",
    "* 1.停止训练,检查load_files,得到的数据,图片和target是一一对应的\n",
    "* 2.减少了训练集大小和epoch和batch_size来加快训练速度\n",
    "* 3.修改了model compile时使用的损失函数,从accuracy 改成binary_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经过查找原因,发现是数据集太小,模型无法收敛了导致的\n",
    "#### 解决方案:\n",
    "* 使用云服务器,有aws和gcp等几个可以选择,我使用gcp继续进行模型的训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在经过训练后,我又发现了上次类似的问题,验证集数据准确率只有0.51左右.最先考虑的情况是epoch太少,所以选择10-->30,发现作用不大\n",
    "#### 问题原因:\n",
    "* 数据集未经过预处理,且预处理的方法是InceptionResNet50独有的,这点需要注意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过训练.发现上面两图中的Loss在epoch=5的时候就到达了最大.Accurcy差不多是最高的,所以判断epoch<10内模型就应该收敛了,于是重新训练\n",
    "# 并且将输入尺寸从224 * 224  改成模型默认的299 * 299 希望成绩变好\n",
    "epochs=10\n",
    "history= model.fit(x_train_data,y_train,validation_data=(x_valid_data,y_valid),epochs=epochs,batch_size=batch_size,callbacks=call_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载权重,进行fine-tune\n",
    "model.load_weights(check_point_filepath)\n",
    "# 记得先编译\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
